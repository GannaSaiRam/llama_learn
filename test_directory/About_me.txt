Recently, I have worked with ABC on a project where I needed to focus on optimizing ETL processes. What I needed to do was to get Amazon website ads reports, download them, and process them as quickly as possible. Due to the increasing number of profiles, it became mandatory to optimize the process. To address this, we initially attempted async process which we were using before. However, even the asynchronous process was consuming too much memory and time to handle the 2,000 to 4,000 profiles efficiently.
Moreover, we encountered 429 errors while fetching the ads report. The formatting of the report itself was taking a significant amount of time, and we had to check every five minutes if the report was completed. Despite this, the report was not finishing on time due to multiple asynchronous calls resulting in a 429 error. To resolve this issue, we implemented a semaphore to limit the number of concurrent requests to around 6,000 to 7,000. This helped ensure that at any given time, only a maximum of 7,000 processes were running in the background, preventing the occurrence of the 429 error.
Additionally, we tackled the memory issue caused by processing numerous reports and profiles by fixing the memory and semaphore sizes. By setting limits on the memory and semaphore sizes, we prevented memory overflows. This approach enabled us to effectively manage the processing of data without encountering memory constraints.
Another design choice I made was to streamline the data flow by first transferring the data to Redshift. Initially, we stored the data in Postgres locally and then transferred it to S3 before finally copying it to Redshift. This process ensured that the data from Amazon was securely stored in Redshift. However, we realized that the multiple levels of data copying were time-consuming and led to unnecessary data redundancy. To address this, we decided to directly push the report data to S3 to avoid redundant data storage. We then utilized Amazon Athena to query the data directly from S3, eliminating the need for multiple data transfers.
Furthermore, instead of processing the report data daily, we shifted our focus to event-based data processing. By capturing data based on events as they occurred, we were able to streamline the data processing and reduce redundancy in the data storage process. These design changes have significantly improved the efficiency and effectiveness of our data processing workflow.